{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from vllm import LLM, SamplingParams\n",
    "from vllm.lora.request import LoRARequest\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain.llms.vllm import VLLM\n",
    "import evaluate\n",
    "import json\n",
    "from langchain_core.prompts import (\n",
    "  ChatPromptTemplate,\n",
    "  SystemMessagePromptTemplate,\n",
    "  HumanMessagePromptTemplate\n",
    ")\n",
    "import os\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define system prompt and human message\n",
    "\n",
    "model_paths = [\"/data/models/huggingface/meta-llama/Llama-2-7b-chat-hf\"]\n",
    "system_template = \"You are a helpful assistant.\"\n",
    "messages = [\n",
    "  SystemMessagePromptTemplate.from_template(system_template),\n",
    "  HumanMessagePromptTemplate.from_template('Paragraph: {question}')\n",
    "]\n",
    "prompt = ChatPromptTemplate.from_messages(messages)\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-20 21:11:24,221\tINFO worker.py:1724 -- Started a local Ray instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 21:11:27 llm_engine.py:72] Initializing an LLM engine with config: model='/data/models/huggingface/meta-llama/Llama-2-7b-chat-hf', tokenizer='/data/models/huggingface/meta-llama/Llama-2-7b-chat-hf', tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=4, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, seed=0)\n",
      "INFO 04-20 21:11:38 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m INFO 04-20 21:11:38 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\n",
      "INFO 04-20 21:11:43 llm_engine.py:322] # GPU blocks: 18067, # CPU blocks: 2048\n",
      "INFO 04-20 21:11:45 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "INFO 04-20 21:11:45 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m INFO 04-20 21:11:45 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m INFO 04-20 21:11:45 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\n",
      "\u001b[36m(RayWorkerVllm pid=3375822)\u001b[0m INFO 04-20 21:11:38 custom_all_reduce.py:125] NVLink detection failed with message \"Not Supported\". This is normal if your machine has no NVLink equipped\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/ray-logging.html#log-deduplication for more options.)\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m [W CUDAGraph.cpp:145] Warning: Waiting for pending NCCL work to finish before starting graph capture. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 04-20 21:12:10 custom_all_reduce.py:199] Registering 715 cuda graph addresses\n",
      "INFO 04-20 21:12:10 model_runner.py:698] Graph capturing finished in 25 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m INFO 04-20 21:12:10 custom_all_reduce.py:199] Registering 715 cuda graph addresses\n",
      "\u001b[36m(RayWorkerVllm pid=3375504)\u001b[0m INFO 04-20 21:12:10 model_runner.py:698] Graph capturing finished in 25 secs.\n",
      "\u001b[36m(RayWorkerVllm pid=3375822)\u001b[0m INFO 04-20 21:11:45 model_runner.py:632] Capturing the model for CUDA graphs. This may lead to unexpected consequences if the model is not static. To run the model in eager mode, set 'enforce_eager=True' or use '--enforce-eager' in the CLI.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=3375822)\u001b[0m INFO 04-20 21:11:45 model_runner.py:636] CUDA graphs can take additional 1~3 GiB memory per GPU. If you are running out of memory, consider decreasing `gpu_memory_utilization` or enforcing eager mode. You can also reduce the `max_num_seqs` as needed to decrease memory usage.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(RayWorkerVllm pid=3375656)\u001b[0m INFO 04-20 21:12:10 custom_all_reduce.py:199] Registering 715 cuda graph addresses\n",
      "\u001b[36m(RayWorkerVllm pid=3375656)\u001b[0m INFO 04-20 21:12:10 model_runner.py:698] Graph capturing finished in 25 secs.\n"
     ]
    }
   ],
   "source": [
    "# Now Loading LLama\n",
    "sampling_params = SamplingParams(temperature=0, top_p=0.9, top_k=10, max_tokens=4096)\n",
    "model = VLLM(model=model_paths[0], gpu_memory_utilization = 0.95, tensor_parallel_size=4, sampling_params = sampling_params)\n",
    "chain = prompt | model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare Data\n",
    "\n",
    "file_path =\"alpaca_format.json\"\n",
    "# load processed marvel data set\n",
    "with open(file_path, 'r') as file:\n",
    "# Parse the JSON file and convert it into a Python dictionary\n",
    "    data = json.load(file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.97it/s]\n"
     ]
    }
   ],
   "source": [
    "# Test on the first sample\n",
    "\n",
    "q = data[0]['instruction']\n",
    "pair = {\"question\":q}\n",
    "\n",
    "answer = chain.invoke(pair)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "The new police chief will have to work hard to restore the public's trust in the department, which has been severely tested in recent months.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rouge1': 0.21621621621621623, 'rouge2': 0.05555555555555556, 'rougeL': 0.21621621621621623, 'rougeLsum': 0.21621621621621623}\n"
     ]
    }
   ],
   "source": [
    ">>> rouge = evaluate.load('rouge')\n",
    ">>> predictions = [answer]\n",
    ">>> references = [data[0]['expected_output']]\n",
    ">>> results = rouge.compute(predictions=predictions,\n",
    "...                         references=references)\n",
    ">>> print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm_personality",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
